%\title{Overleaf Memo Template}
% Using the texMemo package by Rob Oakes
\documentclass[a4paper,11pt]{texMemo}
\usepackage[english]{babel}
%\usepackage[utf8]{inputenc}
\usepackage{graphicx, lipsum}
\usepackage{amssymb,amsthm,amsmath}
\usepackage{array}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}

\input{../CSDA/preamble}

\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newcommand\xylabellarge[4]{% #1 Image, #2=XText, #3=YText,, #4=Title,
  \setlength{\unitlength}{13cm}%
  \begin{picture}(1,0.6)%
    \put(0.04,0.025){\includegraphics[width=0.95\unitlength]{#1}}
    \put(0.54,0){\makebox[0cm]{\small#2}}
    \put(0,0.3){\rotatebox{90.0}{\makebox[0cm]{\small#3}}}
    \put(0.525,0.6){\makebox[0cm]{#4}}
  \end{picture}%
}
\newcommand\xylabelsquare[4]{% #1 Image, #2=XText, #3=YText,, #4=Title,
  \setlength{\unitlength}{6.5cm}%
  \begin{picture}(1,1)%
    \put(0.075,0.075){\includegraphics[width=0.90\unitlength]{#1}}
    \put(0.575,0){\makebox[0cm]{\small#2}}
    \put(0,0.525){\rotatebox{90.0}{\makebox[0cm]{\small#3}}}
    \put(0.525,1){\makebox[0cm]{#4}}
  \end{picture}%
}
\newcommand\medxylabelsquare[4]{% #1 Image, #2=XText, #3=YText,, #4=Title,
  \setlength{\unitlength}{0.33\linewidth}%
  \begin{picture}(1,1)%
    \put(0.075,0.075){\includegraphics[width=0.90\unitlength]{#1}}
    \put(0.525,0){\makebox[0cm]{\small#2}}
    \put(0,0.525){\rotatebox{90.0}{\makebox[0cm]{\small#3}}}
    \put(0.525,1){\makebox[0cm]{#4}}
  \end{picture}%
}
\newcommand\smallxylabelsquare[4]{% #1 Image, #2=XText, #3=YText,, #4=Title,
  \setlength{\unitlength}{0.25\linewidth}%
  \begin{picture}(1,1)%
    \put(0.10,0.10){\includegraphics[width=0.80\unitlength]{#1}}
    \put(0.5,0){\makebox[0cm]{\small#2}}
    \put(0,0.5){\rotatebox{90.0}{\makebox[0cm]{\small#3}}}
    \put(0.525,0.95){\makebox[0cm]{#4}}
  \end{picture}%
}




%% Edit the header section here. To include your
%% own logo, upload a file via the files menu.
\memoto{Erricos Kontoghiorghes}
\memofrom{Chiquet, Grandvalet, Ambroise}
\memosubject{Response to CSDA review CSDA-D-17-00692  (e199)}
\memodate{\today}
\logo{\includegraphics[width=0.25\textwidth]{logo_lamme.pdf}}

\begin{document}
\maketitle
Dear editor,

We are grateful to the reviewers for their careful reading  of
our paper entitled

\begin{center}
  ''Sparsity by Worst-case Penalties''
\end{center}

and their enriching remarks. This letter aims to provide a
point-by-point response to their comments and suggestions.

\section*{Referee 1}

%* Reviewer 1
%***

\begin{itemize}
\item
  \emph{The algorithm starts from a sparse initial guess, i.e., the active
set $A = \emptyset$.  I think it may be useful to discuss briefly how the
computational time changes for an experimental setup, in which the
choice of the initial guess is randomized.}



****  Réponse (dans la lettre) :Yves:
Yves: 
L'algorithme est efficace du fait que nous partons de rien.
L'efficacité de l'algo repose sur le fait que nous ne résolvons pas de gros système et une solution randomisée serait couteuse

Julien:
Si l'on part d'une solution proche de l'optimale effectivement on résout peu de systèmes... 

Yves: 
Une partie de la réponse est dans le warm start que nous avons utilisé dans le protocole....

**** Réponse (à ajouter dans le texte) :Yves:
Dire que c'est important de partir de zéro 

\item
%*** In section 5,
\emph{I would suggest to switch the results on simulated settings and on real-world
data, such that the latter can be commented in light of the properties and
conclusions obtained with synthetic data.}

We have switched the results
on simulated setting and real-world data.
The real-world data indeed exhibits a  difference in support recovery
between \texttt{quadrupen} and \texttt{glmnet}.
This difference was expected as shown in the simulation comparing 
\texttt{quadrupen} \texttt{glmnet} with the  default value of
precision. 


%**** Réponse (faisable) :Christophe:

%OK on fait 
\item 
\emph{In the simulated data section, 
  I would suggest to add the F-measure between
  the support of the true coefficient
  vector $\betaβ^⋆$ and the estimated one $\hat{\beta}$,
  as a performance measure to evaluate the
  selection properties of the different algorithms
  (see Section IV in Gasso et al., 2009).
}

We followed the recommandation and  improve the illustration of difference in support recovery (Figure 8) in using True Postive and False Negative and F-measure (with the corresponding citation of \cite{gasso2009recovering}):

% \begin{verbatim}
% @article{gasso2009recovering,
%   title={Recovering sparse signals with a certain family of nonconvex penalties and DC programming},
%   author={Gasso, Gilles and Rakotomamonjy, Alain and Canu, St{\'e}phane},
%   journal={IEEE Transactions on Signal Processing},
%   volume={57},
%   number={12},
%   pages={4686--4698},
%   year={2009},
%   publisher={IEEE}
% }
% \end{verbatim}

\item 
\emph{In line 244, %:Christophe: :DONE:
   the reference should not be in parenthesis.}

Done.









\item 
*** Figure 6 could be improved, :Julien:
as the axes labels are not easily readable and colors are not
distinguishable.

\item
*** In line 377,                                                       :Yves:
**** Quadrupen, Spam\_lars sont des méthodes du second ordre et ne sont pas sensible aux corrélation... au 
I would suggest to explain the reason why quadrupen, SPAMS-LARS and lars are not
sensitive to the level of correlation between features.


\item
*** In the caption of Figure 8, specify the x-axis. :Julien: 

**** C'est un log-10 ($\lambda_1$)


\end{itemize}
%Gasso G., Rakotomamonjy A., Canu S. (2009). Recovering sparse signals with a certain family of non convex penalties and DC programming. IEEE Transactions on Signal Processing, 57(12), 4686-4698.




\section*{Referee 2}

%The authors proposed a new optimization algorithm to solve the sparse linear regression
%with a norm penalty term, including group-lasso and elastic-net. The proposed method
%is both accurate in estimation and extremely computationally fast, as shown in extensive

%simulation studies. It is very good that the authors also provided the accompanying R-
%package (quadrupen) on CRAN.
%
%The paper is interesting in computational aspects and fit the theme of the journal.
%However, I think a few concerns should be addressed before publication is possible.

\subsection*{Major comments}
\begin{itemize}
\item
  *** 1. As the authors also admitted in the paper, the proposed algorithm is highly similar
to the active-set methods, except the procedures taken in Step 3. I understand
that the authors argue that quadrupen is more flexible in handling a wider range
of penalties and look at the problem from a different perspective. However, either
methodological innovations or numerical improvement should be proved to make the
contribution enough to consider it as an ”alternative” algorithm.

**** Réponse dans le texte 
Step 3 speeds up the algorithm .... that the main advantage....



\item 
*** 2. In the simulation studies, under the same tuning parameter, a more conservative
selection of the quadrupen was observed in the QTL example comparing to that of
glmnet, but not in the examples showed in Figure 8. I was wondering if this is an
universal phenomenon? If it is the authors should provide some theoretical insights
on why does this happen and maybe run some more monte carlo simulations to verify
it, since basically they are solving the same underlying optimization problem.

**** Pour répondre à cela nous montrons les FP et TN et pour s'apercevoir que c'est toujours le cas....
**** Ajouter une phrase commentaire sur la figure 8   :Yves:

glmnet garde les petits coefficients qui ne coutent pas cher dans l'objectif

\item 
*** 3. For solving the elastic-net problem, the authors compared the proposed method to
other two optimization strategies. It seems that the proposed method only gains
obvious efficiency in the regime where λ1 and λ2 are very small. In the case if
we choose parameters probably (according the theoretical rate or simply by cross-
validation), what is the advantage of using quadrupen?

**** Préciser ce que l'on fait sur la figure 6 ... 

**** Pas seulement sur les petites valeurs,
c'est vrai que c'est là qu'il y
le plus de différence mais pour
des valeurs de lambda1 lambda2


**** Mais c'est justement 
sur les petites valeurs que c'est coûteux.
\end{itemize}

\subsection*{Minor comments}

\begin{itemize}
\item
  *** 1. The presentation of the figures in the paper is not very straightforward to readers. For
example, I found Figure 1,2,3,6 kind of hard to understand by reading the captions.
More basic introductions should be made in the captions or in the context describing
the figures. Moreover, please pay attention to the size of fonts in the labs, titles and
legends across the figures. Currently they are quite differing.


**** Plus gros caption :Yves:

\item 
*** 2. In the section 5.2.3 and 5.2.4, the authors benchmarked the methods by Lasso, letting
λ2 = 0. Is it possible to consider the more general elastic-net problem (say for a fixed
λ2) for at least some of the packages, especially in measuring the model selection
accuracy? I understand that the authors have pointed out that they have different
rules in determining tuning parameters.

**** Répondre dans la lettre et le papier on ne parcours pas les mêmes solutions de la même manière.

Fixer lambda puis alpha pour glmnet
Faire bouger lambda1 et lambda2 pour nous

\end{itemize}


\section*{Referee 3}

%The article proposes a novel algorithm, which supposedly solves the elastic- net 
%problem and approximates the solutions to LASSO and l∞,1 version of group LASSO. 
%The idea seems interesting but based on the current version of the article it is 
%difficult to assess the properties and correctness of the pro- posed algorithm.
% Its description is very sketchy. Also, the paper lacks the results on its convergency.
% Additionally, many parts of the paper suffer from the lack of precision and contain wrong statements or unjustified claims. 
%The list of detailed remarks is included below.


\begin{itemize}

\item 
\emph{1. Equations (4) and the one above (4) give the wrong impression
  concerning the ''simplicity'' of the solution of problem (3).  Namely,
  the solution of the ''inner'' problem (maximizing over $\gamma$ or minimizing
  over $\beta$) will depend on the value of the second parameter, which
  makes the ''outer'' optimization problem rather difficult.
}

Equations 4 and above are actually simple but pose an optimization problem difficult to solve. They highlight that when $\gamma$ is known the optimization problem becomes simple. This type of circularity  lies at the heart of the Expectation Maximization algorithm. We use a similar  form in the  relation between $\beta$ and $\gamma$ to solve the problem of external maximization without pretending that the global problem is a simple problem.


% **** Réponse dans la lettre: c'est comme les équation d'EM où les équations d'update sont simples et le problème globale est
% complexe :Christophe:

% Les équations 4 et au dessus sont effectivement simple mais pose un problème d'optimisation difficile à résoudre. Elles en évidence que lorsque $\gamma$ est connu le problème d'optimisation devient simple. Ce type  de circularité du problème est au coeur de l'algorithme Expectation Maximisation.  Nous utilisons la forme de la relation pour résoudre le problème de maximisation externe sans prétendre que le problème global est un problème simple. 


% **** On explique à la fin du paragraphe:
% et, defined by the extreme points of the convex polytope B∗1. This number of
% 87 points typically increases exponentially in p, but, with the working-set strategy,
% 88 the number of configuration actually visited typically grows linearly with the
% 89 number of non-zero coefficients in the solution βˆ.


\item
  \emph{2.  It is not true that the quadratic problem (5) is a
    different formulation of (3).
    Under some circumstances it only asymptotically (for
    $\eta \rightarrow \infty$) approximates (3).  In Section 3 the
    authors have shown that this asymptotic approximation works for
    LASSO and for $\ell_\infty$, 1 version of group LASSO. Their
    derivations strongly rely on the simple form of $\ell_1$ or
    $\ell_\infty$ balls.
  }

**** We agree with the reviewer, as stated in the paper line 100

\item 
  \emph{It is hard to verify the authors statement that similar asymptotic approximations would work for other norms,  like e.g.
    the Sorted L-One Norm (see OSCAR \citep{Bondell08} or SLOPE \citep{bogdan2015slope}).
    If this assertion is indeed true,
    I suggest to add a Section with a proper mathematical justification.}

%**** Ajouter OSCAR dans la lettre.                               :Christophe:
%Voir oscar.tex

%\subsection{OSCcAR}

To avoid overloading the article, we prefer to leave aside the approximations for many other possible algorithms. In the following, we add an example concerning the OSCAR algorithm to show the principle of extending our approach to another penalty.

  
\input{oscar}



\item 
*** 3. I suggest to mathematically formalize the Section 2.4 
on the Geomet- rical Interpretation. Specifically,
 it is not clear at all that the solution belongs to the intersection of all the balls centered at γ ∈ B⋆η. 

**** On ne comprend pas ce qu'il ne comprend pas... réponse détaillé de Yves

Note, that for small c and large η, this intersection would be an empty set.

**** Oui mais on ne l'a pas posé comme cela... car c'est écrit sous forme lagrangienne et c'est celle là qui nous intéresse
La vue géométrique est intéressante pour l'interprétation.... 


\item
  \emph{Also, please, note that $\hat{\beta}$ depends on $\gamma$,
      thus maximizing $$||\beta(\gamma) − \gamma||_1$$
does not seem to be a simple task.}

%**** Dans la lettre:                                             :Christophe:
We agree with the reviewer that it is not a simple task,
as stated in the response of the first remark. 

%this is why we adopt an alternate optimization point of view... It is an EM algo


\item 
On the other hand the statement that for each β, γˆ(β) belongs 
to the set of extreme points does not seem to require a special justification.

**** Dans l'article : changer les captions des figures 1 et 2  :Yves:

Figures 1 and 2 require an extended description.

 It is not clear what is represented by different colors.
 Also, according to the description of Figure 1,
 it contains the graphs for Elastic Net, l∞ and OSCAR, while according to the description in the text
 the first two graphs contain LASSO, and l∞,1 version of group LASSO.

**** Dans la figure 1 on représente des versions L2 ifées :Yves:

 Where are crosses in graph number 4 in Figure 2 ?
**** A l'extérieur de la figure :Yves:



\item 
*** 4. The formulation (5) is asymptotically equivalent to LASSO when η → ∞. 
It would be good to provide some results (at least empirical) 
to show how the accuracy of this approximation depends on η.
Should ”good” η depend on p, the correlation structure or the sparsity of the signal ?

****  Réponse dans la lettre:  :Yves:
LA formulation 5 permet de mélanger L2 et autre pénalité lorsqu'il n'y pas de L2 : 
We always use formulation 3 to solve the problem


le lambda qui tend vers l'infini c'est une façon de voir les


\item 
*** 5. It is not clear at all if the first two steps of the algorithm converge. 
Note that γˆ(β) is selected as the extreme point, which is most distant to β. 
On the other hand the quadratic penalty used to estimate βˆ(γˆ) privileges β close to γˆ. 
For large λ, when the penalty dominates the first term in the objective function, 
βˆ(γˆ) will be very close to γˆ and γˆ will not longer be coherent (most distant) to βˆ.
I would welcome some mathematical result illustrating the convergence of the first two steps of the algorithm.


**** Prouver la convergence de l'algo

\item 
*** 6. It would be good to explain what is the meaning of gj. 

Did not we select γ as the most distant extreme point ?
**** Yes but  in the active set

Why do we calculate gj by minimizing over all points from the dual ball ?

**** We need to consider gj to add a variable in the active set

\item
  \emph{7. I do not quite see how to formulate a stopping criterion based on Proposition 1.}
  
**** upper bound est le terme de droite :Yves:


\item
\emph{In the algorithm the worst case gradients are computed for $\gamma$ from the ball,
while the Proposition 1 requires $\gamma$  to be in the closed
complement of the ball.
}

**** :Yves: dans la lettre

**** Expliquer que l'algo produit une séquence de gamma non admissible et c'est pour ces solutions 
que nous cherchons à avoir une borne sur l'optimum du critère...

\item 
\emph{8. Real Data Analysis - comparison of LASSO and quadrupen.
There is no information which $\eta$ was used here. }

**** We do not use eta see above :Christophe:


\item
\emph{
It is quite natural that glmnet and quadrupen give different results since quadrupen is only the
approximation to LASSO. }

**** Not true, it is the same problem thanks to formulation 3

How do these two algorithms compare in execution time for this relatively large data set ?

**** 

\item
*** 9. The synthetic data examples are rather small (p = 100).
It would be good to see the results when p can reach several thousands (say 5000).
 Which η was used for quadrupen ?

**** 

The authors use LARS as the benchmark.
 Please, note that LARS is only an approximation to LASSO.
 For example LARS paths are mono- tonic, 
which is not true about the LASSO path (here variables may appear and disappear many times along the path). 
Thus, in general, LARS does not solve the LASSO optimization problem and should not be considered as the true optimum.

**** We use LAR the package implements the lasso :Julien:


Instead of comparing D(method), which is never negative,
even if a given method is better than LARS, 
I suggest to directly compare the values of the objective function.


**** 
\end{itemize}

\bibliographystyle{../CSDA/elsarticle-harv} 
\bibliography{../CSDA/biblio_crafter}



\end{document}
