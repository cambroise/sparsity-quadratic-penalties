%\title{Overleaf Memo Template}
% Using the texMemo package by Rob Oakes
\documentclass[a4paper,11pt]{texMemo}
\usepackage[english]{babel}
%\usepackage[utf8]{inputenc}
\usepackage{graphicx, lipsum}
\usepackage{amssymb,amsthm,amsmath}
\usepackage{array}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}

\input{../CSDA/preamble}

\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newcommand\xylabellarge[4]{% #1 Image, #2=XText, #3=YText,, #4=Title,
  \setlength{\unitlength}{13cm}%
  \begin{picture}(1,0.6)%
    \put(0.04,0.025){\includegraphics[width=0.95\unitlength]{#1}}
    \put(0.54,0){\makebox[0cm]{\small#2}}
    \put(0,0.3){\rotatebox{90.0}{\makebox[0cm]{\small#3}}}
    \put(0.525,0.6){\makebox[0cm]{#4}}
  \end{picture}%
}
\newcommand\xylabelsquare[4]{% #1 Image, #2=XText, #3=YText,, #4=Title,
  \setlength{\unitlength}{6.5cm}%
  \begin{picture}(1,1)%
    \put(0.075,0.075){\includegraphics[width=0.90\unitlength]{#1}}
    \put(0.575,0){\makebox[0cm]{\small#2}}
    \put(0,0.525){\rotatebox{90.0}{\makebox[0cm]{\small#3}}}
    \put(0.525,1){\makebox[0cm]{#4}}
  \end{picture}%
}
\newcommand\medxylabelsquare[4]{% #1 Image, #2=XText, #3=YText,, #4=Title,
  \setlength{\unitlength}{0.33\linewidth}%
  \begin{picture}(1,1)%
    \put(0.075,0.075){\includegraphics[width=0.90\unitlength]{#1}}
    \put(0.525,0){\makebox[0cm]{\small#2}}
    \put(0,0.525){\rotatebox{90.0}{\makebox[0cm]{\small#3}}}
    \put(0.525,1){\makebox[0cm]{#4}}
  \end{picture}%
}
\newcommand\smallxylabelsquare[4]{% #1 Image, #2=XText, #3=YText,, #4=Title,
  \setlength{\unitlength}{0.25\linewidth}%
  \begin{picture}(1,1)%
    \put(0.10,0.10){\includegraphics[width=0.80\unitlength]{#1}}
    \put(0.5,0){\makebox[0cm]{\small#2}}
    \put(0,0.5){\rotatebox{90.0}{\makebox[0cm]{\small#3}}}
    \put(0.525,0.95){\makebox[0cm]{#4}}
  \end{picture}%
}




%% Edit the header section here. To include your
%% own logo, upload a file via the files menu.
\memoto{Erricos Kontoghiorghes}
\memofrom{Chiquet, Grandvalet, Ambroise}
\memosubject{Response to CSDA review CSDA-D-17-00692  (e199)}
\memodate{\today}
\logo{\includegraphics[width=0.25\textwidth]{logo_lamme.pdf}}



\newenvironment{comment}
{
   \par\medskip \color{black}%
   \textbf{Response: }}
{\medskip}
 
\newenvironment{remark}
{\begin{itshape} \color{gray}}
{\end{itshape}}


  
\begin{document}
\maketitle
Dear editor,

We are grateful to the reviewers for their careful reading  of
our paper entitled

\begin{center}
  ''Sparsity by Worst-case Penalties''
\end{center}

and their enriching remarks. This letter aims to provide a
point-by-point response to their comments and suggestions.

\section*{Referee 1}

%* Reviewer 1
%***

\begin{itemize}
\item
  \begin{remark}
    The algorithm starts from a sparse initial guess, i.e., the active
    set $A = \emptyset$.  I think it may be useful to discuss briefly
    how the computational time changes for an experimental setup, in
    which the choice of the initial guess is randomized.
  \end{remark}

  \begin{comment}

****  Réponse (dans la lettre) :Yves:
Yves: 
L'algorithme est efficace du fait que nous partons de rien.
L'efficacité de l'algo repose sur le fait que nous ne résolvons pas de gros système et une solution randomisée serait couteuse

Julien:
Si l'on part d'une solution proche de l'optimale effectivement on résout peu de systèmes... 

Yves: 
Une partie de la réponse est dans le warm start que nous avons utilisé dans le protocole....

**** Réponse (à ajouter dans le texte) :Yves:
Dire que c'est important de partir de zéro 

  \end{comment}

\item
%*** In section 5,
  \begin{remark}
    I would suggest to switch the results on simulated settings and on
    real-world data, such that the latter can be commented in light of
    the properties and conclusions obtained with synthetic data.
  \end{remark}

  \begin{comment}
    We have switched the results on simulated setting and real-world
    data.  The real-world data indeed exhibits a difference in support
    recovery between \texttt{quadrupen} and \texttt{glmnet}.  This
    difference was expected as shown in the simulation comparing
    \texttt{quadrupen} \texttt{glmnet} with the default value of
    precision.
 \end{comment}

%**** Réponse (faisable) :Christophe:

%OK on fait 
\item 
  \begin{remark}
    In the simulated data section, I would suggest to add the
    F-measure between the support of the true coefficient vector
    $\betaβ^⋆$ and the estimated one $\hat{\beta}$, as a performance
    measure to evaluate the selection properties of the different
    algorithms (see Section IV in Gasso et al., 2009).
 \end{remark}

 \begin{comment}
   We followed the recommandation and improve the illustration of
   difference in support recovery (Figure 8) in using True Postive and
   False Negative and F-measure (with the corresponding citation of
   \cite{gasso2009recovering}).
 \end{comment}
% \begin{verbatim}
% @article{gasso2009recovering,
%   title={Recovering sparse signals with a certain family of nonconvex penalties and DC programming},
%   author={Gasso, Gilles and Rakotomamonjy, Alain and Canu, St{\'e}phane},
%   journal={IEEE Transactions on Signal Processing},
%   volume={57},
%   number={12},
%   pages={4686--4698},
%   year={2009},
%   publisher={IEEE}
% }
% \end{verbatim}

\item 
  \begin{remark}
    In line 244, %:Christophe: :DONE:
   the reference should not be in parenthesis.
  \end{remark}

  \begin{comment}
    Done.
  \end{comment}




\item 
  \begin{remark}
    Figure 6 could be improved, as the axes labels are not easily
    readable and colors are not distinguishable.
\end{remark}

\begin{comment}
  :Julien:
\end{comment}



\item
   \begin{remark}
     In line 377, I would suggest to explain the reason why quadrupen,
     SPAMS-LARS and lars are not sensitive to the level of correlation
     between features.
    \end{remark}

\begin{comment}
  :Yves: Quadrupen, Spam\_lars sont des méthodes du second ordre et ne
  sont pas sensible aux corrélation... au
\end{comment}


\item
  \begin{remark}
    In the caption of Figure 8, specify the x-axis.
  \end{remark}
  
\begin{comment}
    :Julien: 
**** C'est un log-10 ($\lambda_1$)
\end{comment}

\end{itemize}
%Gasso G., Rakotomamonjy A., Canu S. (2009). Recovering sparse signals with a certain family of non convex penalties and DC programming. IEEE Transactions on Signal Processing, 57(12), 4686-4698.




\section*{Referee 2}

%The authors proposed a new optimization algorithm to solve the sparse linear regression
%with a norm penalty term, including group-lasso and elastic-net. The proposed method
%is both accurate in estimation and extremely computationally fast, as shown in extensive

%simulation studies. It is very good that the authors also provided the accompanying R-
%package (quadrupen) on CRAN.
%
%The paper is interesting in computational aspects and fit the theme of the journal.
%However, I think a few concerns should be addressed before publication is possible.

\subsection*{Major comments}
\begin{itemize}
\item

  \begin{remark}
    1. As the authors also admitted in the paper, the proposed
    algorithm is highly similar to the active-set methods, except the
    procedures taken in Step 3. I understand that the authors argue
    that quadrupen is more flexible in handling a wider range of
    penalties and look at the problem from a different
    perspective. However, either methodological innovations or
    numerical improvement should be proved to make the contribution
    enough to consider it as an ”alternative” algorithm.
\end{remark}
\begin{comment}
**** Réponse dans le texte 
Step 3 speeds up the algorithm .... that the main advantage....
\end{comment}


\item
  \begin{remark}
    2. In the simulation studies, under the same tuning parameter, a
    more conservative selection of the quadrupen was observed in the
    QTL example comparing to that of glmnet, but not in the examples
    showed in Figure 8. I was wondering if this is an universal
    phenomenon? If it is the authors should provide some theoretical
    insights on why does this happen and maybe run some more monte
    carlo simulations to verify it, since basically they are solving
    the same underlying optimization problem.
\end{remark}

\begin{comment}
**** Pour répondre à cela nous montrons les FP et TN et pour s'apercevoir que c'est toujours le cas....
**** Ajouter une phrase commentaire sur la figure 8   :Yves:

glmnet garde les petits coefficients qui ne coutent pas cher dans l'objectif
\end{comment}
\item 
  \begin{remark}
    3. For solving the elastic-net problem, the authors compared the
    proposed method to other two optimization strategies. It seems
    that the proposed method only gains obvious efficiency in the
    regime where $\lambda_1$ and $\lambda_2$ are very small.
    In the case if we choose
    parameters probably (according the theoretical rate or simply by
    cross- validation), what is the advantage of using quadrupen?
\end{remark}


\begin{comment}

  **** Préciser ce que l'on fait sur la figure 6 ... 

**** Pas seulement sur les petites valeurs,
c'est vrai que c'est là qu'il y
le plus de différence mais pour
des valeurs de lambda1 lambda2


**** Mais c'est justement 
sur les petites valeurs que c'est coûteux.

\end{comment}
\end{itemize}

\subsection*{Minor comments}

\begin{itemize}
\item
  \begin{remark}
    1. The presentation of the figures in the paper is not very
    straightforward to readers. For example, I found Figure 1,2,3,6
    kind of hard to understand by reading the captions.  More basic
    introductions should be made in the captions or in the context
    describing the figures. Moreover, please pay attention to the size
    of fonts in the labs, titles and legends across the
    figures. Currently they are quite differing.
  \end{remark}

  \begin{comment}
**** Plus gros caption :Yves:
  \end{comment}
\item 
  \begin{remark}
    2. In the section 5.2.3 and 5.2.4, the authors benchmarked the
    methods by Lasso, letting $\lambda_2 = 0$. Is it possible to consider the
    more general elastic-net problem (say for a fixed $\lambda_2$) for at least
    some of the packages, especially in measuring the model selection
    accuracy? I understand that the authors have pointed out that they
    have different rules in determining tuning parameters.
  \end{remark}
  \begin{comment}
    **** Répondre dans la lettre et le papier on ne parcours pas les
    mêmes solutions de la même manière.

    Fixer lambda puis alpha pour glmnet Faire bouger lambda1 et
    lambda2 pour nous
  \end{comment}
\end{itemize}


\section*{Referee 3}

%The article proposes a novel algorithm, which supposedly solves the elastic- net 
%problem and approximates the solutions to LASSO and l∞,1 version of group LASSO. 
%The idea seems interesting but based on the current version of the article it is 
%difficult to assess the properties and correctness of the pro- posed algorithm.
% Its description is very sketchy. Also, the paper lacks the results on its convergency.
% Additionally, many parts of the paper suffer from the lack of precision and contain wrong statements or unjustified claims. 
%The list of detailed remarks is included below.


\begin{itemize}

\item 
  \begin{remark}
    1. Equations (4) and the one above (4) give the wrong impression
  concerning the ''simplicity'' of the solution of problem (3).  Namely,
  the solution of the ''inner'' problem (maximizing over $\gamma$ or minimizing
  over $\beta$) will depend on the value of the second parameter, which
  makes the ''outer'' optimization problem rather difficult.
   \end{remark}

   \begin{comment}
     Equations 4 and above are actually simple but pose an
     optimization problem difficult to solve. They highlight that when
     $\gamma$ is known the optimization problem becomes simple. This
     type of circularity lies at the heart of the Expectation
     Maximization algorithm. We use a similar form in the relation
     between $\beta$ and $\gamma$ to solve the problem of external
     maximization without pretending that the global problem is a
     simple problem.
\end{comment}

% **** Réponse dans la lettre: c'est comme les équation d'EM où les équations d'update sont simples et le problème globale est
% complexe :Christophe:

% Les équations 4 et au dessus sont effectivement simple mais pose un problème d'optimisation difficile à résoudre. Elles en évidence que lorsque $\gamma$ est connu le problème d'optimisation devient simple. Ce type  de circularité du problème est au coeur de l'algorithme Expectation Maximisation.  Nous utilisons la forme de la relation pour résoudre le problème de maximisation externe sans prétendre que le problème global est un problème simple. 


% **** On explique à la fin du paragraphe:
% et, defined by the extreme points of the convex polytope B∗1. This number of
% 87 points typically increases exponentially in p, but, with the working-set strategy,
% 88 the number of configuration actually visited typically grows linearly with the
% 89 number of non-zero coefficients in the solution βˆ.


\item
  \begin{remark}
    2.  It is not true that the quadratic problem (5) is a different
    formulation of (3).  Under some circumstances it only
    asymptotically (for $\eta \rightarrow \infty$) approximates (3).
    In Section 3 the authors have shown that this asymptotic
    approximation works for LASSO and for $\ell_\infty$, 1 version of
    group LASSO. Their derivations strongly rely on the simple form of
    $\ell_1$ or $\ell_\infty$ balls.
   \end{remark}

   \begin{comment}
     We agree with the reviewer, as stated in the paper line 100
    \end{comment}

   
\item 
  \begin{remark}
    It is hard to verify the authors statement that similar asymptotic
    approximations would work for other norms, like e.g.  the Sorted
    L-One Norm (see OSCAR \citep{Bondell08} or SLOPE
    \citep{bogdan2015slope}).  If this assertion is indeed true, I
    suggest to add a Section with a proper mathematical justification.
\end{remark}
  
%**** Ajouter OSCAR dans la lettre.                               :Christophe:
%Voir oscar.tex

%\subsection{OSCcAR}
\begin{comment}
  To avoid overloading the article, we prefer to leave aside the
  approximations for many other possible algorithms. In the following,
  we add an example concerning the OSCAR algorithm to show the
  principle of extending our approach to another penalty.

  
\input{oscar}

\end{comment}


\item 
  \begin{remark}
    3. I suggest to mathematically formalize the Section 2.4 on the
    Geometrical Interpretation. Specifically, it is not clear at all
    that the solution belongs to the intersection of all the balls
    centered at $\gamma$.
  \end{remark}

\begin{comment}
  **** On ne comprend pas ce qu'il ne comprend pas... réponse détaillé
  de Yves

Note, that for small c and large η, this intersection would be an empty set.

**** Oui mais on ne l'a pas posé comme cela... car c'est écrit sous forme lagrangienne et c'est celle là qui nous intéresse
La vue géométrique est intéressante pour l'interprétation.... 

\end{comment}



\item
   \begin{remark}
     Also, please, note that $\hat{\beta}$ depends on $\gamma$, thus
     maximizing $$||\hat{\beta}(\gamma) -  \gamma||$$ does not seem to be a
     simple task.
   \end{remark}

   \begin{comment}
     % **** Dans la lettre: :Christophe:
     We agree with the reviewer that it is not a simple task, as
     stated in the response of the first remark.
     % this is why we adopt an alternate optimization point of
     % view... It is an EM algo
\end{comment}


\item
     \begin{remark}
       On the other hand the statement that for each β, γˆ(β) belongs
       to the set of extreme points does not seem to require a special
       justification.
     \end{remark}

   \begin{comment}
  *** Dans l'article : changer les captions des figures 1 et 2 :Yves:
   \end{comment}

\item
  \begin{remark}
    Figures 1 and 2 require an extended description.

    It is not clear what is represented by different colors.  Also,
    according to the description of Figure 1, it contains the graphs
    for Elastic Net, $\ell_\infty$ and OSCAR, while according to the
    description in the text the first two graphs contain LASSO, and
    $\ell_{1,\infty}$ version of group LASSO.
  \end{remark}

  \begin{comment}
  **** Dans la figure 1 on représente des versions L2 ifées :Yves:
\end{comment}

 \begin{remark}
   Where are crosses in graph number 4 in Figure 2 ?
 \end{remark}
 
 \begin{comment}
   In this particular case, the crosses are outside the figure
   frame. Scaling the figure to show the crosses would make the figure
   unreadable.
\end{comment}

\item
  \begin{remark}
    4. The formulation (5) is asymptotically equivalent to LASSO when
    $\eta \leftarrow \infty$.  It would be good to provide some results (at least
    empirical) to show how the accuracy of this approximation depends
    on  $\eta$.  Should ''good'' $\eta$ depend on $p$, the correlation structure or
    the sparsity of the signal ?
  \end{remark}
  \begin{comment}
****  Réponse dans la lettre:  :Yves:
LA formulation 5 permet de mélanger L2 et autre pénalité lorsqu'il n'y pas de L2 : 
We always use formulation 3 to solve the problem


le lambda qui tend vers l'infini c'est une façon de voir les
\end{comment}

\item 
  \begin{remark}
    5. It is not clear at all if the first two steps of the algorithm
    converge.  Note that $\hat{\gamma}(\beta)$ %γˆ(β)
    is selected as the extreme point, which is most distant to
    $\beta$.  On the other hand the quadratic penalty used to estimate
    $\hat{\beta}(\hat{\gamma})$ %βˆ(γˆ)
    privileges $\beta$ close to $\hat{\gamma}$.  For large $\lambda$,
    when the penalty dominates the first term in the objective
    function, $\hat{\beta}(\hat{\gamma})$ will be very close to
    $\hat{\gamma}$ and $\gamma$ $\hat{\gamma}$ will not longer be
    coherent (most distant) to $\hat{\beta}$.  I would welcome some
    mathematical result illustrating the convergence of the first two
    steps of the algorithm.
  \end{remark}

  \begin{comment}
    **** Prouver la convergence de l'algo
  \end{comment}
\item 
  \begin{remark}
   6. It would be good to explain what is the meaning of $g_j$. 
   Did not we select $\gamma$ as the most distant extreme point ?
\end{remark}

\begin{comment}
**** Yes but  in the active set
\end{comment}


\begin{remark}
Why do we calculate $g_j$ by minimizing over all points from the dual ball ?
\end{remark}
\begin{comment}
**** We need to consider $g_j$  to add a variable in the active set
\end{comment}
\item
  \begin{remark}
  7. I do not quite see how to formulate a stopping criterion based on Proposition 1.
\end{remark}

\begin{comment}
**** upper bound est le terme de droite :Yves:

\end{comment}
\item
  \begin{remark}
In the algorithm the worst case gradients are computed for $\gamma$ from the ball,
while the Proposition 1 requires $\gamma$  to be in the closed
complement of the ball.

\end{remark}
\begin{comment}
**** :Yves: dans la lettre

**** Expliquer que l'algo produit une séquence de gamma non admissible et c'est pour ces solutions 
que nous cherchons à avoir une borne sur l'optimum du critère...
\end{comment}
\item 
  \begin{remark}
    8. Real Data Analysis - comparison of LASSO and quadrupen.  There
    is no information which $\eta$ was used here.
  \end{remark}

  \begin{comment}
**** We do not use eta see above :Christophe:
\end{comment}
\item

  \begin{remark}
It is quite natural that glmnet and quadrupen give different results since quadrupen is only the
approximation to LASSO. 
\end{remark}
\begin{comment}
**** Not true, it is the same problem thanks to formulation 3
\end{comment}

\begin{remark}
  How do these two algorithms compare in execution time for this
  relatively large data set ?
\end{remark}

\item
  \begin{remark}
    9. The synthetic data examples are rather small ($p = 100$).  It
    would be good to see the results when $p$ can reach several
    thousands (say 5000).  Which $\eta$ was used for quadrupen ?
\end{remark}


\begin{remark}
  The authors use LARS as the benchmark.  Please, note that LARS is
  only an approximation to LASSO.  For example LARS paths are
  monotonic, which is not true about the LASSO path (here variables
  may appear and disappear many times along the path).  Thus, in
  general, LARS does not solve the LASSO optimization problem and
  should not be considered as the true optimum.
\end{remark}

\begin{comment}
**** We use LAR the package implements the lasso :Julien:
\end{comment}


\begin{remark}
Instead of comparing D(method), which is never negative,
even if a given method is better than LARS, 
I suggest to directly compare the values of the objective function.
\end{remark}


\begin{comment}
  ****
\end{comment}

\end{itemize}

\bibliographystyle{../CSDA/elsarticle-harv} 
\bibliography{../CSDA/biblio_crafter}



\end{document}
