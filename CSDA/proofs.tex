\section{Proof of Proposition~\ref{prop:monitoring}}~\label{sec:proof:prop:monitoring}

We detail here a proof yielding a slightly tighter bound. 
Proposition~\ref{prop:monitoring} is simply a corollary of
Proposition~\ref{prop:monitoring:appendix:v2} stated and proved below.

The following Lemma relates the penalty
associated with a infeasible $\bfgamma$-value 
($\norm[*]{\bfgamma} > \eta$) to the one obtained by shrinking this
$\bfgamma$-value to reach the boundary of $\uball[*]^\eta$.

\begin{lemma}\label{prop:lemma1:v2}
  Let $\clS \subseteq \{1,...,p\}$, $\alpha\in(0,1)$, $\norm[*]{\cdot}$ be a vectorial norm and $\varphi^{*}(\cdot,\clS,\alpha) :
  \mathbb{R}^{p} \rightarrow \mathbb{R}^{p}$ be defined as follows:
  \begin{equation*} 
    \left\{
    \begin{array}{l}
      \varphi_{\clS}^{*}(\bfgamma,\clS,\alpha) = \bfgamma_{\clS} \\
      \varphi_{\clS^c}^{*}(\bfgamma,\clS,\alpha) = \alpha \bfgamma_{\clS^c} 
    \end{array}\right.\enspace.
   \end{equation*} 
%    For any $0\leq \alpha \leq 1$,
   Then,
   \begin{equation}\label{eq:lemma1:v2}
     \norm{\bfbeta-\varphi^{*}(\bfgamma,\clS,\alpha)}^2 \geq 
      \alpha \norm{\bfbeta-\bfgamma}^2  -
      \alpha (1-\alpha) \norm{\bfgamma_{\clS^c}}^{2} 
     \enspace. 
  \end{equation}
  \begin{proof}
    \begin{align*}
      \norm{\bfbeta-\varphi^{*}(\bfgamma,\clS,\alpha)}^2 & =
      \norm{\bfbeta_{\clS}-\bfgamma_{\clS}}^2 +
      \alpha \norm{\bfbeta_{\clS^c}-\bfgamma_{\clS^c}}^2 +
      (1-\alpha) \norm{\bfbeta_{\clS^c}}^{2} -
      \alpha (1-\alpha) \norm{\bfgamma_{\clS^c}}^{2} \\
      & \geq
      \norm{\bfbeta_{\clS}-\bfgamma_{\clS}}^2 +
      \alpha \norm{\bfbeta_{\clS^c}-\bfgamma_{\clS^c}}^2  -
      \alpha (1-\alpha) \norm{\bfgamma_{\clS^c}}^{2}  \\
      & \geq
      \alpha \norm{\bfbeta-\bfgamma}^2  -
      \alpha (1-\alpha) \norm{\bfgamma_{\clS^c}}^{2} 
      \enspace.
    \end{align*}
  \end{proof}
%   Note that the gap in the last inequality is 
%   $\norm{\bfbeta}^2({\norm[*]{\bfgamma}-\norm[*]{\bfgamma'}})/{\norm[*]{\bfgamma}}$,
%   so that the gap in Inequality \eqref{eq:lemma1:v2} is
%   $\norm{\bfbeta}^2({\norm[*]{\bfgamma}-\norm[*]{\bfgamma'}})/{\norm[*]{\bfgamma'}}$.
\end{lemma}
%
\begin{proposition}\label{prop:monitoring:appendix:v2}
  For any vectorial norm $\norm[*]{\cdot}$, 
%   when 
%   $\uball[*]^\eta$ is defined as $\uball[*]^\eta=\left\{\bfgamma \in \mathbb{R}^{p}:
%                       \norm[*]{\bfgamma} \leq \eta \right\}$, then, 
  $\forall \bfgamma \in \mathbb{R}^{p}:\norm[*]{\bfgamma} \geq \eta$, and 
  $\forall (\clS,\alpha)\in2^{\{1,\ldots,p\}}\times(0,1)$ such that 
  $\norm[*]{(\bfgamma_{\clS},\alpha\bfgamma_{\clS^c })}\leq \eta$, we   
  have:
  \begin{equation*}
    \min_{\bfbeta\in\mathbb{R}^{p}} \max_{\bfgamma' \in \uball[*]^\eta} 
    J_\lambda(\bfbeta,\bfgamma') 
    \geq
    \alpha J_\lambda\left(\bfbeta^\star\left(\bfgamma\right),\bfgamma\right) -
    \lambda \alpha (1-\alpha) \norm{\bfgamma_{\clS^c}}^{2}
    \enspace,
  \end{equation*}
  where 
  \begin{equation*}
    J_\lambda(\bfbeta,\bfgamma) = \norm{\bfX \bfbeta - \bfy}^2 + 
      \lambda \norm{\bfbeta - \bfgamma}^2
    \enspace \text{and} \enspace
    \bfbeta^\star(\bfgamma) = \argmin_{\bfbeta\in\mathbb{R}^{p}} J_\lambda(\bfbeta,\bfgamma)
    \enspace.
  \end{equation*}
  \begin{proof} 
  For all $\bfbeta \in \mathbb{R}^{p}$ and for any $(\bfgamma,\clS,\alpha) \in
  \mathbb{R}^{p}\times2^{\{1,\ldots,p\}}\times(0,1)$ such that
  $\varphi^{*}(\bfgamma,\clS,\alpha) \in \uball[*]^\eta$, with $\varphi^{*}$ 
  defined as in Lemma \ref{prop:lemma1:v2} we have
  \begin{equation}\label{eq:maxgreater:v2}
    \max_{\bfgamma' \in \uball[*]^\eta} 
    J_\lambda(\bfbeta,\bfgamma') 
    \geq
    J_\lambda\left(\bfbeta,\varphi^{*}(\bfgamma,\clS,\alpha)\right)
    \enspace,
  \end{equation}
  since $\varphi^{*}(\bfgamma,\clS,\alpha)$ belongs to $\uball[*]^\eta$. 
  We now compute a lower bound of the right-hand-side for $\bfgamma$ such that
  $\norm[*]{\bfgamma} \geq \eta$:
    \begin{align}       
      J_\lambda\left(\bfbeta,\varphi^{*}(\bfgamma,\clS,\alpha)\right) 
        & = \alpha \left( 
              \frac{1}{\alpha} \norm{\bfX \bfbeta - \bfy}^2 + 
              \frac{\lambda}{\alpha} 
              \norm{\bfbeta - \varphi^{*}(\bfgamma,\clS,\alpha)}^2
            \right)  
            \nonumber \\
        & \geq \alpha \left( 
              \norm{\bfX \bfbeta - \bfy}^2 + 
              \frac{\lambda}{\alpha} 
              \norm{\bfbeta - \varphi^{*}(\bfgamma,\clS,\alpha)}^2
            \right)  
            \label{eq:crude_inequality} \\
        & \geq \alpha \left( 
              \norm{\bfX \bfbeta - \bfy}^2 + 
              \lambda \norm{\bfbeta - \bfgamma}^2
           \right) -
           \lambda\alpha (1-\alpha) \norm{\bfgamma_{\clS^c}}^{2} 
           \enspace, \nonumber
    \end{align}
    where the last inequality stems from Lemma~\ref{prop:lemma1:v2}.
    This inequality holds for any given $\bfbeta$-value, in particular for
    $\bfbeta^\star(\varphi^{*}(\bfgamma,\clS,\alpha))=\argmin_{\bfbeta \in \mathbb{R}^{p}}
    J_\lambda\left(\bfbeta,\varphi^{*}(\bfgamma,\clS,\alpha)\right)$:
    \begin{align}       
      \min_{\bfbeta\in\mathbb{R}^{p}} 
      J_\lambda\left(\bfbeta,
                     \varphi^{*}(\bfgamma,\clS,\alpha)
               \right) 
      & \geq \alpha
          J_\lambda\left(\bfbeta^\star\left(\varphi^{*}(\bfgamma,\clS,\alpha)\right)\right) -
          \lambda\alpha (1-\alpha) \norm{\bfgamma_{\clS^c}}^{2} 
          \nonumber \\
      & \geq \alpha
          J_\lambda\left(\bfbeta^\star\left(\bfgamma\right),\bfgamma\right) -
          \lambda\alpha (1-\alpha) \norm{\bfgamma_{\clS^c}}^{2} 
          \label{eq:inequality:particular:appendix:v2} 
          \enspace,
    \end{align}
    where the second inequality follows from the definition of
    $\bfbeta^\star(\bfgamma)$.
    Inequality \eqref{eq:inequality:particular:appendix:v2} can be restated as:
     \begin{align*}       
       \min_{\bfbeta\in\mathbb{R}^{p}} 
       J_\lambda\left(\bfbeta,\varphi^{*}(\bfgamma,\clS,\alpha)\right)
       & \geq
         \alpha \min_{\bfbeta\in\mathbb{R}^{p}} 
         J_\lambda(\bfbeta,\bfgamma)  -
         \lambda\alpha (1-\alpha) \norm{\bfgamma_{\clS^c}}^{2} 
       \enspace.
    \end{align*}
    We finally remark that,  
    since $\varphi^{*}(\bfgamma,\clS,\alpha) \in \uball[*]^\eta$,
    we trivially have:
    \begin{align*}
      \min_{\bfbeta\in\mathbb{R}^{p}} \max_{\bfgamma \in \uball[*]^\eta} J_\lambda(\bfbeta,\bfgamma) 
      & \geq 
       \min_{\bfbeta\in\mathbb{R}^{p}} 
        J_\lambda\left(\bfbeta,\varphi^{*}(\bfgamma,\clS,\alpha)\right)
     \enspace,
    \end{align*}
    which concludes the proof.
  \end{proof}
\end{proposition}
Proposition~\ref{prop:monitoring} follows by choosing $\alpha={\eta}/{\norm[*]{\bfgamma}}$.

% \iffalse
%   Let $J$ be the objective function of the generic
%   Problem~\eqref{eq:robust:general:form3}, we trivially have that, for any
%   $\bfgamma$-value,
%   $\min_{\bfbeta\in\mathbb{R}^{p}} \max_{\bfgamma \in \uball[*]^\eta}
%   J(\bfbeta,\bfgamma) \geq \min_{\bfbeta\in\mathbb{R}^{p}} J(\bfbeta,\bfgamma)$.
%   %
%   Hence, a tight optimality gap can be computed provided one can guess a
%   near-optimal $\bfgamma$-value.
%   When entering Step \ref{item:algo:step3} of Algorithm~\ref{algo:active_set}, the current best guess
%   $\widehat\bfgamma_{\clS}$ has to be completed on the complement $\clS^c$.
%   We propose to do so by minimizing the magnitude of the gradient with respect to $\bfbeta$ at the current solution:
%   \begin{align*}
%     \widehat\bfgamma_{\clS^c} & = \argmin_{\bfgamma_{\clS^c}:\bfgamma \in \uball[*]^\eta}
%                                \left|  \bfX_{\centerdot\supp^c}^\intercal(\bfX_{\centerdot\supp}\bfbeta_{\supp}-\bfy) - 
%                                \lambda \bfgamma_{\supp^c} \right|
%       \enspace.
%   \end{align*}
%   The resulting vector $\widehat\bfgamma$ is then used to compute the lower
%   bound on the optimal objective function as $\min_{\bfbeta\in\mathbb{R}^{p}} J(\bfbeta,\widehat\bfgamma)$. 
%   This lower bound, which requires to solve a linear system of size $p$, is
%   much lighter to compute than the duality gap \citep[see details
%   in][]{2012_FML_Bach}, and was observed to provide a tight bound (as a sanity
%   check, note that when $\bfbeta$ is optimal, our estimated optimality gap is null).

% %

% \begin{align*}
%   \bfbeta_\supp & = \argmin_{\bfbeta\in\mathbb{R}^{|\supp|}} 
%                   \norm{\bfX_{\centerdot\supp} \bfbeta - \bfy}^2 + \lambda_1 \norm[1]{\bfbeta}
%   \\
%   \widehat\bfbeta_{\supp^c} & = \bfzero \\ 
%   \widehat\bfgamma_{\supp^c} & = \argmin_{\bfgamma_{\supp^c} \in [-1,1]^{|\supp^c|}}
%                            \left|  \bfX_{\centerdot\supp^c}^\intercal(\bfX\widehat\bfbeta-\bfy) + 
%                              \lambda_1 \bfgamma_{\supp^c} \right|
%     \enspace. 
% \end{align*}
% We can then compute 
% \begin{align*}
%   \widetilde{\bfbeta} & = \argmin_{\bfbeta\in\mathbb{R}^{p}} 
%                   \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1 \bfbeta^\intercal\widehat\bfgamma
%     \enspace. 
% \end{align*}
% We then have:
% \begin{align*}
% J(\bfbeta^\star) & = \min_{\bfbeta\in\mathbb{R}^p} 
%   \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1  \norm[1]{\bfbeta} \\
%   &= \min_{\bfbeta\in\mathbb{R}^p} \max_{\bfgamma \in \uball[*]^\eta}
%       \Big\{ \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1 \bfbeta^\intercal\bfgamma \Big\} \\
%   &= \max_{\bfgamma \in \uball[*]^\eta} \min_{\bfbeta\in\mathbb{R}^p}
%       \Big\{ \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1 \bfbeta^\intercal\bfgamma \Big\} \\
%   & \geq \min_{\bfbeta\in\mathbb{R}^p}
%       \Big\{ \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1 \bfbeta^\intercal\widehat\bfgamma \Big\}  \\
%   & = \norm{\bfX \widetilde{\bfbeta} - \bfy}^2 + \lambda_1 \widetilde{\bfbeta}^{(t)^\intercal}\widehat\bfgamma
%   \enspace,
% \end{align*}
% which provides a lower bound for $J(\bfbeta^\star)$.
% Meanwhile, we obviously have:
% \begin{align*}
% J(\bfbeta^\star) & = \min_{\bfbeta\in\mathbb{R}^p} 
%   \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1 \norm[1]{\bfbeta} \\
%   &\leq \norm{\bfX \widehat\bfbeta - \bfy}^2 + \lambda_1  \norm[1]{\widehat\bfbeta} 
%   \enspace,
% \end{align*}
% So that $J(\bfbeta^\star)$ is easily bracketed during the optimization process. 
% Note that when $\bfbeta_\supp=\bfbeta^\star$, we have that
% $\widetilde{\bfbeta}=\bfbeta_\supp$.



% % \ifverylong

% The additional computational cost for monitoring (that is, the cost of evaluating
% the upper and lower bounds for an intermediate solution can be important because
% $\widetilde{\bfbeta}$ requires to solve a linear system of size $p$\ldots 
% We could do better than that if we can prove that $\big(\widetilde{\bfbeta}\big)_j=0$ if
% the subgradient at  $\widehat\bfbeta_{\supp^c}$ comprises zero. In this situation, we 
% should only consider adding the variables for which the optimality conditions
% are violated.

% \subsection{Monitoring Convergence: Lasso-Case}

% \begin{align*}
%   \bfbeta_\supp & = \argmin_{\bfbeta\in\mathbb{R}^{|\supp|}} 
%                   \norm{\bfX_{\centerdot\supp} \bfbeta - \bfy}^2 + \lambda_1 \norm[1]{\bfbeta}
%   \\
%   \widehat\bfbeta_{\supp^c} & = \bfzero \\ 
%   \widehat\bfgamma_{\supp^c} & = \argmin_{\bfgamma_{\supp^c} \in [-1,1]^{|\supp^c|}}
%                            \left|  \bfX_{\centerdot\supp^c}^\intercal(\bfX\widehat\bfbeta-\bfy) + 
%                              \lambda_1 \bfgamma_{\supp^c} \right|
%     \enspace. 
% \end{align*}
% We can then compute 
% \begin{align*}
%   \widetilde{\bfbeta} & = \argmin_{\bfbeta\in\mathbb{R}^{p}} 
%                   \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1 \bfbeta^\intercal\widehat\bfgamma
%     \enspace. 
% \end{align*}
% We then have:
% \begin{align*}
% J(\bfbeta^\star) & = \min_{\bfbeta\in\mathbb{R}^p} 
%   \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1  \norm[1]{\bfbeta} \\
%   &= \min_{\bfbeta\in\mathbb{R}^p} \max_{\bfgamma \in \uball[*]^\eta}
%       \Big\{ \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1 \bfbeta^\intercal\bfgamma \Big\} \\
%   &= \max_{\bfgamma \in \uball[*]^\eta} \min_{\bfbeta\in\mathbb{R}^p}
%       \Big\{ \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1 \bfbeta^\intercal\bfgamma \Big\} \\
%   & \geq \min_{\bfbeta\in\mathbb{R}^p}
%       \Big\{ \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1 \bfbeta^\intercal\widehat\bfgamma \Big\}  \\
%   & = \norm{\bfX \widetilde{\bfbeta} - \bfy}^2 + \lambda_1 \widetilde{\bfbeta}^{(t)^\intercal}\widehat\bfgamma
%   \enspace,
% \end{align*}
% which provides a lower bound for $J(\bfbeta^\star)$.
% Meanwhile, we obviously have:
% \begin{align*}
% J(\bfbeta^\star) & = \min_{\bfbeta\in\mathbb{R}^p} 
%   \norm{\bfX \bfbeta - \bfy}^2 + \lambda_1 \norm[1]{\bfbeta} \\
%   &\leq \norm{\bfX \widehat\bfbeta - \bfy}^2 + \lambda_1  \norm[1]{\widehat\bfbeta} 
%   \enspace,
% \end{align*}
% So that $J(\bfbeta^\star)$ is easily bracketed during the optimization process. 
% Note that when $\bfbeta_\supp=\bfbeta^\star$, we have that
% $\widetilde{\bfbeta}=\bfbeta_\supp$.
% % \fi
% \fi

% \input{codelikeproof}

