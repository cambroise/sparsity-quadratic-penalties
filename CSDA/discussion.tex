\section{Discussion}

%% SUMMARY

% PRINCIPLE
This paper presents a new viewpoint on sparsity-inducing penalties
where the dual norms associated with these penalties play a central role. 
Technically, our formulation is a simple dual form of the original problem. 
However, we do not follow a very general principle such as Fenchel duality: 
we specifically tailor our formulation to optimisation problems involving a 
sparsity-inducing norm.
The dual variables define a series of linear or quadratic penalties whose 
sublevel sets define the feasible set of the original problem through
intersection.

This  viewpoint  enables  to  encompass in the same framework  several
well-known penalties. In particular, we detailed how the solutions to the Lasso and the
group-Lasso (with the $\ell_{\infty,1}$  mixed norm), possibly applied
together with an  $\ell_2$ ridge penalty (leading to what  is known as
the elastic net for the Lasso) can be derived. 

% ALGORITHM AND MAGIC BOUND
We derived a  general-purpose algorithm that computes the solution to
the penalized regression problem,  and proposed a new lower
bound  on the  minimum  of  the objective  function to
assess convergence.  The proposed  algorithm solves a series of
quadratic  problems on a working set defined  by the dual  variables.   
It has  been
thoroughly tested  and compared with  state-of-the-art implementations
for the elastic net and the Lasso, and prevails over its competitors for 
the problems tested, involving  up to a few thousands of variables.

% DISCUSSION
From a  practical viewpoint, an  important feature of our  approach is
that it solves the original problem  up to machine precision: we shown
that when  variable selection  is involved,  optimization with  a high
level of precision is mandatory to recover the true model.
% Moreover, our algorithm  is well suited for computing  a solution path
% rather than  for just one  value of $\lambda_1, (\lambda_2)$  since we
% are  generally interested  in cross-validating  these paths  for model
% selection purpose (at least in genomics).
\\

% FUTURE DEV
Regarding  future  development,  the   algorithm  can  be  adapted  to
non-quadratic loss  functions for  addressing other  learning problems
such  as  classification,  but  this  generalization,  which  requires
solving non-quadratic  problems, may not  be as efficient  compared to
the  existing alternatives.   We are  now examining  how to  address a
wider range of penalties by extending the framework in two directions:
first,  to accommodate  additional general  $\ell_2$ penalties  in the
form of  arbitrary symmetric  positive semidefinite matrix  instead of
the simple ridge, in particular to provide an efficient implementation
of the  structured elastic  net \citep{2010_AOS_Slawski} ;  second, we
plan to  derive similar  views on a  wider range  of sparsity-inducing
penalties, such as the fused-Lasso or the OSCAR \citep{Bondell08}.
